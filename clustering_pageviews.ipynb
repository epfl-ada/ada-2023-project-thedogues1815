{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching the main fake news based on COVID-19 misinformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_wikipedia_sub_subheadings(article_title):\n",
    "    # Replace spaces with underscores for Wikipedia URL format\n",
    "    article_title_url = article_title.replace(' ', '_')\n",
    "\n",
    "    # Wikipedia base URL\n",
    "    base_url = 'https://en.wikipedia.org/wiki/'\n",
    "\n",
    "    # Full URL to scrape\n",
    "    full_url = base_url + article_title_url\n",
    "\n",
    "    # Send a GET request to the Wikipedia page\n",
    "    response = requests.get(full_url)\n",
    "\n",
    "    # Check if the page was retrieved successfully\n",
    "    if response.status_code == 200:\n",
    "        # Parse the content of the request with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # DataFrame to store the headings, subheadings, sub-subheadings, and links\n",
    "        headings_df = pd.DataFrame(columns=['Sub-subheading', 'Subheading', 'Main Heading', 'Links'])\n",
    "\n",
    "        # Initialize current headings\n",
    "        current_main_heading = None\n",
    "        current_sub_heading = None\n",
    "        current_sub_subheading = None\n",
    "        index = 0\n",
    "\n",
    "        # Iterate through all heading tags\n",
    "        for tag in soup.find_all(['h2', 'h3', 'h4', 'p', 'a']):\n",
    "            # Remove the '[edit]' part from the text\n",
    "            text = tag.get_text().replace('[edit]', '').strip()\n",
    "            # Determine tag type and process accordingly\n",
    "            if tag.name == 'h2':\n",
    "                current_main_heading = text\n",
    "            elif tag.name == 'h3':\n",
    "                current_sub_heading = text\n",
    "                # Reset sub-subheading when a new subheading is found\n",
    "                current_sub_subheading = None\n",
    "                # Add the subheading to the dataframe\n",
    "                headings_df.loc[index] = [text, current_sub_heading, current_main_heading, []]\n",
    "                index += 1\n",
    "            elif tag.name == 'h4':\n",
    "                current_sub_subheading = text\n",
    "                # Add the sub-subheading to the dataframe\n",
    "                headings_df.loc[index] = [text, current_sub_heading, current_main_heading, []]\n",
    "                index += 1\n",
    "            elif tag.name == 'a' and 'href' in tag.attrs:\n",
    "                # Check if the link is a valid article link\n",
    "                link = tag['href']\n",
    "                if link.startswith('/wiki/') and ':' not in link:\n",
    "                    # Add the link to the appropriate heading in the dataframe\n",
    "                    if current_sub_subheading:\n",
    "                        headings_df.loc[headings_df['Sub-subheading'] == current_sub_subheading, 'Links'].apply(lambda x: x.append(link))\n",
    "                    elif current_sub_heading:\n",
    "                        headings_df.loc[headings_df['Subheading'] == current_sub_heading, 'Links'].apply(lambda x: x.append(link))\n",
    "        return headings_df\n",
    "    else:\n",
    "        # Return an empty DataFrame if the page could not be retrieved\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Example usage\n",
    "article_title = \"COVID-19 misinformation\"\n",
    "headings_df = scrape_wikipedia_sub_subheadings(article_title)\n",
    "headings_df.head()  # Display the first few rows of the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking existence in 2020 for use with mobility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/wiki_pageviews_covid-master/data/topics_linked.csv', 'r') as file:\n",
    "    topics = csv.reader(file)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the page view counts of groups of info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_pageview_count(language, articles):\n",
    "    api_url = \"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/{project}/{access}/{agent}/{article}/{granularity}/{start}/{end}\"\n",
    "\n",
    "    params = {\n",
    "        \"project\": f\"{language}.wikipedia\",   # Language-specific Wikipedia project\n",
    "        \"access\": \"all-access\",\n",
    "        \"agent\": \"user\",\n",
    "        \"granularity\": \"monthly\",\n",
    "        \"start\": \"20180101\",\n",
    "        \"end\": \"20230101\"\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"WikiWackyNews\"\n",
    "    }\n",
    "\n",
    "    pageviews_data = {}\n",
    "\n",
    "    for article in articles:\n",
    "        params[\"article\"] = article\n",
    "\n",
    "        # Make the API request\n",
    "        response = requests.get(api_url.format(**params), headers=headers)\n",
    "        \n",
    "        # Access the data and save it as a dataframe\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            item_list = data.get(\"items\", [])\n",
    "            \n",
    "            if item_list:\n",
    "                df = pd.DataFrame(item_list).copy()\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'], format='%Y%m%d%H')\n",
    "                pageviews_data[article] = df\n",
    "            else:\n",
    "                print(f\"No data available for {article}\")\n",
    "        else:\n",
    "            print(f\"Error fetching data for {article}. Status Code: {response.status_code}\")\n",
    "\n",
    "    return pageviews_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "language = \"en\"\n",
    "articles_list = [\"Covid-19\", \"Hydroxychloroquine\"]\n",
    "result = fetch_pageview_count(language, articles_list)\n",
    "\n",
    "# result contains a dictionary where keys are article names and values are DataFrames with pageviews data.\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
